{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91867718-c283-4d96-8f6a-ae227b2f5cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73091d05-1257-48fd-8a6b-111a06544e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when encount the word ‘ went , ’ the model , have learn pattern , predict the base form as ‘ go. ’ similarli , for ‘ happier , ’ the model deduc ‘ happi ’ as the lemma . the advantag lie in the model ’ s abil to adapt to vari linguist nuanc and handl irregular , make it robust for lemmat divers vocabulari .'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'When encountering the word ‘went,’ the model, having learned patterns, predicts the base form as ‘go.’ Similarly, for ‘happier,’ the model deduces ‘happy’ as the lemma. The advantage lies in the model’s ability to adapt to varied linguistic nuances and handle irregularities, making it robust for lemmatizing diverse vocabularies.'\n",
    "st = stemming_lemma_reprocess(text)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3baff3-de0c-4b43-ac9b-da3aec788cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When encountering the word ‘ went , ’ the model , having learned pattern , predicts the base form a ‘ go. ’ Similarly , for ‘ happier , ’ the model deduces ‘ happy ’ a the lemma . The advantage lie in the model ’ s ability to adapt to varied linguistic nuance and handle irregularity , making it robust for lemmatizing diverse vocabulary .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = stemming_lemma_reprocess(text, type_select='lemma')\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79093a9d-687d-47ae-af9c-3546da90ebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_label</th>\n",
       "      <th>medical_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Tissue changes around loose prostheses. A cani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neuropeptide Y and neuron-specific enolase lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sexually transmitted diseases of the colon, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Lipolytic factors associated with murine and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Does carotid restenosis predict an increased r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition_label                                   medical_abstract\n",
       "0                5  Tissue changes around loose prostheses. A cani...\n",
       "1                1  Neuropeptide Y and neuron-specific enolase lev...\n",
       "2                2  Sexually transmitted diseases of the colon, re...\n",
       "3                1  Lipolytic factors associated with murine and h...\n",
       "4                3  Does carotid restenosis predict an increased r..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../Medical-Abstracts-TC-Corpus/medical_tc_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09241c57-9cae-494a-8b71-69547739fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df['medical_abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e23102-e2de-48bc-90eb-62eb5b78686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = ' '.join(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae71a173-5927-47ff-b79d-138e3803614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'PH);', 'near-diploid', 'suprabasilar', 'Interlocking']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text = list(set(df_text.split(' ')))\n",
    "df_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d8e0e3-fa20-451c-a47d-000fb9fc7ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96222"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997bc8dd-aaea-4a62-9981-569fe2671515",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = [i for i in df_text if len(i)<4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c539960-86bf-4993-92eb-5998490c071b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8be3f872-29d1-43dd-88c6-6a165512d7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'CI' in dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "982b2438-709d-4d74-aec4-d4e52e5aeba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Abbreviation_List_for_Medical_Record_Documentation.txt', 'r', encoding='utf-8') as rt:\n",
    "    ct = rt.read()\n",
    "    ct = ct.split('\\n')[1:]\n",
    "rt.close()\n",
    "len(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70d829b1-acb5-4661-b55d-0847cad692d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_keep = [f.lower() for f in ct]\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "f = [j for j in words_to_keep if j in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27083a1c-ea04-47b7-816e-07b3f271f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss  # For multi-label classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59983142-5fbf-4514-a9bd-8e982962e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0286, grad_fn=<DivBackward1>)\n",
      "tensor(1.1859, grad_fn=<DivBackward1>)\n",
      "tensor(1.1076, grad_fn=<DivBackward1>)\n",
      "tensor(1.8620, grad_fn=<DivBackward1>)\n",
      "tensor(1.4847, grad_fn=<DivBackward1>)\n",
      "tensor(1.2742, grad_fn=<DivBackward1>)\n",
      "tensor(0.8883, grad_fn=<DivBackward1>)\n",
      "tensor(1.7102, grad_fn=<DivBackward1>)\n",
      "tensor(1.3609, grad_fn=<DivBackward1>)\n",
      "tensor(1.0872, grad_fn=<DivBackward1>)\n",
      "tensor(1.0562, grad_fn=<DivBackward1>)\n",
      "tensor(1.2400, grad_fn=<DivBackward1>)\n",
      "tensor(0.9202, grad_fn=<DivBackward1>)\n",
      "tensor(1.2893, grad_fn=<DivBackward1>)\n",
      "tensor(1.1464, grad_fn=<DivBackward1>)\n",
      "tensor(0.9277, grad_fn=<DivBackward1>)\n",
      "tensor(1.1269, grad_fn=<DivBackward1>)\n",
      "tensor(0.8731, grad_fn=<DivBackward1>)\n",
      "tensor(1.0754, grad_fn=<DivBackward1>)\n",
      "tensor(0.8068, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "texts = [\"Lòai chó rất trung thành với con người.\", \"Mèo có thể ăn động vật và thực vật.\", \"Cây xanh mang lại nguồn ô xi quí giá.\", \"Máy móc là một phần của cuộc sống.\"]\n",
    "labels = torch.tensor([[1, 0, 0, 0], [1, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "# Token hóa văn bản\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tạo dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.tokenized_texts['input_ids'][idx], 'attention_mask': self.tokenized_texts['attention_mask'][idx], 'labels': self.labels[idx]}\n",
    "\n",
    "dataset = CustomDataset(tokenized_texts, labels)\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Tạo mô hình BERT\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "# Hàm mất mát BCEWithLogitsLoss\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tối ưu hóa\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Quá trình huấn luyện\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Tính logits từ mô hình\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Tính toán loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "        print(loss)\n",
    "        # Backpropagation và tối ưu hóa\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá mô hình (tương tự trong quá trình huấn luyện)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45102226-3b8a-4632-b850-4fc4f6d94aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_420043/2046010791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(self.labels[idx])  # Create a 1D tensor for each label\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2, 1, 4])) must be the same as input size (torch.Size([2, 4]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Tính toán loss\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Backpropagation và tối ưu hóa\u001b[39;00m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:714\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/nn/functional.py:3148\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3145\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([2, 1, 4])) must be the same as input size (torch.Size([2, 4]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss  # For multi-label classification\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], return_tensors='pt', max_length=self.max_length, truncation=True, padding='max_length')\n",
    "        label = torch.tensor(self.labels[idx])  # Create a 1D tensor for each label\n",
    "        return {'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'labels': label.unsqueeze(0)}  # Add a dimension for batch compatibility\n",
    "\n",
    "# Example data\n",
    "texts = [\"Lòai chó rất trung thành với con người.\", \"Mèo có thể ăn động vật và thực vật.\", \"Cây xanh mang lại nguồn ô xi quí giá.\", \"Máy móc là một phần của cuộc sống.\"]\n",
    "labels = torch.tensor([[1, 0, 0, 0], [1, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create dataset\n",
    "max_length = 20  # Adjust as needed\n",
    "dataset = MultiLabelDataset(texts, labels, tokenizer, max_length)\n",
    "# Initialize BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)  # 4 labels in your case\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# Set up DataLoader\n",
    "batch_size = 2  # Adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Training loop\n",
    "num_epochs = 3  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Tính toán loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "\n",
    "        # Backpropagation và tối ưu hóa\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "318915fe-880f-4f8a-8fa6-6af758ff613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0490210-2a74-4f5d-ad24-ce5f553cf931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition_label</th>\n",
       "      <th>medical_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Tissue changes around loose prostheses. A cani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neuropeptide Y and neuron-specific enolase lev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sexually transmitted diseases of the colon, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Lipolytic factors associated with murine and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Does carotid restenosis predict an increased r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   condition_label                                   medical_abstract\n",
       "0                5  Tissue changes around loose prostheses. A cani...\n",
       "1                1  Neuropeptide Y and neuron-specific enolase lev...\n",
       "2                2  Sexually transmitted diseases of the colon, re...\n",
       "3                1  Lipolytic factors associated with murine and h...\n",
       "4                3  Does carotid restenosis predict an increased r..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Medical-Abstracts-TC-Corpus/medical_tc_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80ffee2d-9b4e-4d65-9cd9-42303be36b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11550, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67ff2eeb-e587-4d4d-a2d2-db7adf5d5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('medical_abstract', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5735465d-2c13-40fc-84f6-ecbaa1bd8107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9445, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e45edbc-77fb-4d09-96f0-95b0c4de81a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2105"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11550-9445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa0257-8c5d-4125-8e68-437e199ff00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

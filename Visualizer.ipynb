{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aca5ef-701f-4b6d-8b57-3e357c717058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from optimizer_loss import *\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Create fake data\n",
    "# num_samples = 100\n",
    "# num_classes = 5\n",
    "\n",
    "# logits = torch.randn(num_samples, num_classes)\n",
    "# labels = torch.randint(0, 2, (num_samples, num_classes)).float()\n",
    "\n",
    "# # Instantiate the CrossEntropyLossMultiLabel\n",
    "# loss_fn = CrossEntropyLossMultiLabel()\n",
    "\n",
    "# # Calculate the loss\n",
    "# loss = loss_fn(logits, labels)\n",
    "\n",
    "# # Plot the sigmoid function\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# logits_range = np.linspace(-5, 5, 100)\n",
    "# sigmoid_values = sigmoid(logits_range)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot sigmoid function\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(logits_range, sigmoid_values, label='Sigmoid Function')\n",
    "# plt.title('Sigmoid Activation Function')\n",
    "# plt.xlabel('Logits')\n",
    "# plt.ylabel('Sigmoid(Logits)')\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot the binary cross-entropy loss\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.scatter(logits.view(-1).detach().numpy(), labels.view(-1).detach().numpy(), label='True Labels')\n",
    "# plt.scatter(logits.view(-1).detach().numpy(), torch.sigmoid(logits).view(-1).detach().numpy(), label='Sigmoid(Logits)')\n",
    "# plt.title('Binary Cross-Entropy Loss Calculation')\n",
    "# plt.xlabel('Logits')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77443707-354b-4dfe-8c8b-29c02017ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3781af-afe6-428a-9ae5-6a46da1f7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo = log_loss([1,0,0,0], [0.8,0.1,0.1, 0])\n",
    "# lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bf45c-1cbf-4359-97c5-a4b6260084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# -(math.log2(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ab8b6-d121-4aec-a532-b8d1ef9da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot of distributions\n",
    "# from matplotlib import pyplot\n",
    "# # define distributions\n",
    "# events = ['red', 'green', 'blue']\n",
    "# p = [0.10, 0.40, 0.50]\n",
    "# q = [0.80, 0.15, 0.05]\n",
    "# print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
    "# # plot first distribution\n",
    "# pyplot.subplot(2,1,1)\n",
    "# pyplot.bar(events, p)\n",
    "# # plot second distribution\n",
    "# pyplot.subplot(2,1,2)\n",
    "# pyplot.bar(events, q)\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5ced4-a19b-4168-9dc1-25c07cf245ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Tạo dữ liệu và vẽ đồ thị\n",
    "x = np.linspace(-7, 7, 500)\n",
    "y = sigmoid(x)\n",
    "# Thêm một số điểm ví dụ\n",
    "example_points = np.array([-4, -2, 0, 2, 4])\n",
    "print(example_points)\n",
    "print(sigmoid(example_points))\n",
    "\n",
    "plt.plot(x, y, label='Sigmoid Function')\n",
    "plt.scatter(example_points, sigmoid(example_points), color='red', label='Example Points')\n",
    "\n",
    "plt.title('Sigmoid Activation Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.axhline(0, color='black',linewidth=0.5)\n",
    "plt.axvline(0, color='black',linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff189d-2029-4788-b1d4-27ef9b1523a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyexcel as p\n",
    "# ccs_insight2 = p.Sheet()\n",
    "# ccs_insight2.name = \"Worldwide Mobile Phone Shipments (Billions), 2017-2021\"\n",
    "# ccs_insight2.ndjson = \"\"\"\n",
    "#                     {\"Compound\": [\"R.T.\", \"QIon\", \"Response\", \"Conc\", \"Units\", \"Dev(Min)\"]}\n",
    "#                     {\"Internal Standards\": [\"\", \"\", \"\", \"\", \"\", \"\"]}\n",
    "#                     {\"1) Toluol D8\": [\"\", \"\", \"\", \"\", \"\", \"\"]}\n",
    "#                     \"\"\".strip()\n",
    "# ccs_insight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4ee2b-afe8-46cf-a072-0a63eed3f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float)\n",
    "labels = torch.tensor([0,1,1,0,0], dtype=torch.float)\n",
    "\n",
    "logits_sigmoid = torch.sigmoid(logits)\n",
    "print(logits_sigmoid)\n",
    "\n",
    "loss = F.binary_cross_entropy(logits_sigmoid, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b88fd-4799-4d00-86b0-b2081856f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor([0, 1, 1, 0, 0], dtype=torch.float)\n",
    "\n",
    "# Using BCEWithLogitsLoss\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "loss = criterion(logits, labels)\n",
    "print(\"Binary Cross Entropy Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Accessing gradients\n",
    "gradients = logits.grad\n",
    "print(\"Gradients:\", gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de63863-de8c-4192-952e-d9f73f0bacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor([0, 1, 1, 0, 0], dtype=torch.float)\n",
    "\n",
    "logits_sigmoid = torch.sigmoid(logits)\n",
    "print(logits_sigmoid)\n",
    "\n",
    "loss = F.binary_cross_entropy(logits_sigmoid, labels)\n",
    "print(\"Binary Cross Entropy Loss:\", loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print('losss: ', loss)\n",
    "# Accessing gradients\n",
    "gradients = logits.grad\n",
    "print(\"Gradients:\", gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665aec0a-316d-492c-811b-f595f99c7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_manual = 1 / (1 + torch.exp(-logits)) - labels\n",
    "gradients_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ab142-6dd1-4d32-9d2a-a4a82c1847f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigt(logit):\n",
    "    return 1 / (1 + math.exp(-logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f14ac-ef84-4abd-82eb-8547c1b7ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(lbl, pre):\n",
    "#     rs = -lbl*math.log(pre) -(1-lbl)*math.log(1-pre)\n",
    "#     print(rs)\n",
    "#     return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5d301-ae38-47f6-93b9-b19c3651e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ave = 0\n",
    "# for i in range(len(logits)):\n",
    "#     ave += forward(labels[i], logits_sigmoid[i])\n",
    "# ave/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525fbf5-6158-44a0-a4bb-499cf6e254e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y_true, y_pred):\n",
    "    rs = y_true*(y_pred-1) + (1-y_true)*(y_pred)\n",
    "    return rs\n",
    "\n",
    "# Gradients: tensor([ 0.0988, -0.0787, -0.0679,  0.0920,  0.1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c48c7-b7f5-4a1f-a74f-04c578a0f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = [ 0.4938, 0.6065, 0.6604, 0.4602, 0.5011]\n",
    "lbl =  [0, 1, 1, 0, 0]\n",
    "\n",
    "devi = 0\n",
    "for i in range(len(fd)):\n",
    "    h =  backward(lbl[i], fd[i])\n",
    "    devi += h\n",
    "    print(h/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27b7b3-381b-4247-a991-fba1419288eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = np.array([0.4938, 0.6065, 0.6604, 0.4602, 0.5011])\n",
    "af = np.array([ 0.4938, -0.3935, -0.3396,  0.4602,  0.5011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b9a37-f072-428a-b6b7-197b0e92cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Hàm Binary Cross Entropy (BCE)\n",
    "# def binary_cross_entropy(y_true, y_pred):\n",
    "#     epsilon = 1e-15\n",
    "#     y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "#     return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# # Đạo hàm của BCE theo tham số y_pred\n",
    "# def bce_gradient(y_true, y_pred):\n",
    "#     epsilon = 1e-15\n",
    "#     y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "#     return - (y_true / y_pred - (1 - y_true) / (1 - y_pred))\n",
    "\n",
    "# # Quá trình gradient descent\n",
    "# def gradient_descent(y_true, learning_rate=0.1, epochs=50):\n",
    "#     y_pred = 0.5  # Giả sử giá trị dự đoán ban đầu\n",
    "#     bce_values = []\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         bce = binary_cross_entropy(y_true, y_pred)\n",
    "#         gradient = bce_gradient(y_true, y_pred)\n",
    "#         y_pred -= learning_rate * gradient  # Cập nhật tham số\n",
    "\n",
    "#         bce_values.append(bce.mean())\n",
    "\n",
    "#     return bce_values\n",
    "\n",
    "# # Tạo dữ liệu giả lập\n",
    "# y_true = np.array([1, 0, 1, 0, 1])\n",
    "# learning_rate = 0.1\n",
    "# epochs = 30\n",
    "\n",
    "# # Thực hiện quá trình gradient descent\n",
    "# bce_values = gradient_descent(y_true, learning_rate, epochs)\n",
    "\n",
    "# # Vẽ đồ thị\n",
    "# plt.plot(range(epochs), bce_values, label='Binary Cross Entropy')\n",
    "# plt.title('Gradient Descent on Binary Cross Entropy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Binary Cross Entropy Value')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cbef9-cb28-4277-b5d8-7f2df8cf868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor([0, 1, 1, 0, 0], dtype=torch.float)\n",
    "\n",
    "logits_sigmoid = torch.sigmoid(logits)\n",
    "print(logits_sigmoid)\n",
    "\n",
    "loss = F.binary_cross_entropy(logits_sigmoid, labels)\n",
    "print(\"Binary Cross Entropy Loss:\", loss.item())\n",
    "# print(\"Binary Cross Entropy Loss:\", loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "gradients = logits.grad\n",
    "print(\"Gradients:\", gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763edc97-90be-4fb1-845c-1b614fa3b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], requires_grad=True)\n",
    "labels = torch.tensor([0., 1., 1., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac55398-6aa2-4e94-b3b4-1674260d8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2.0\n",
    "logits_sigmoid = torch.sigmoid(logits)\n",
    "\n",
    "ce_loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='none')\n",
    "print(ce_loss)\n",
    "\n",
    "pt = torch.exp(-ce_loss)\n",
    "print(pt)\n",
    "loss = (1 - pt) ** gamma * ce_loss\n",
    "print(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923cfa45-b851-4311-a968-c1a707376cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from focal_loss import binary_focal_loss\n",
    "loss = binary_focal_loss([0,1,1,0,0],  [0.4938, 0.6065, 0.6604, 0.4602, 0.5011], gamma=2)\n",
    "np.set_printoptions(precision=3)\n",
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967771-e1db-4f68-837b-fff9a1635e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep\n",
    "import torch\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], requires_grad=True)\n",
    "labels = torch.tensor([0., 1., 1., 0., 0.])\n",
    "def forward(logits, labels):\n",
    "        ''' \n",
    "        Forward pass for Focal Loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Raw outputs from the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Focal Loss.\n",
    "        '''\n",
    "        gamma=2\n",
    "        logits_sigmoid = torch.sigmoid(logits)\n",
    "        logits_flat = logits_sigmoid.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        loss = -((1 - logits_flat) ** gamma) * labels_flat * torch.log(logits_flat) - (1 - labels_flat) * (logits_flat ** gamma) * torch.log(1 - logits_flat)\n",
    "        return loss.mean()\n",
    "d=forward(logits, labels)\n",
    "print(d)\n",
    "d.backward()\n",
    "gradients = logits.grad\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37009fd-dcc5-42e5-aeb9-3b1ed53298bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def focal_loss(pt, gamma):\n",
    "    return -(1 - pt) ** gamma * np.log(pt)\n",
    "\n",
    "pt_values = np.linspace(0.01, 1, 100)\n",
    "gamma_values = [0, 1.0, 1.5, 2.0, 2.5]  # list of Gamma values for check\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    focal_loss_values = focal_loss(pt_values, gamma)\n",
    "    plt.plot(pt_values, focal_loss_values, label=f'Gamma = {gamma}')\n",
    "\n",
    "plt.title('Overview for Different Gamma Values')\n",
    "plt.xlabel('f(Q(x)) - Predicted Probability')\n",
    "plt.ylabel('Focal Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e30354-a079-4672-802a-802241767e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Smoothing Loss\n",
    "\n",
    "labels = torch.tensor([0,1,1,0,0], dtype=torch.float)\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float)\n",
    "\n",
    "smoothing = 0.3\n",
    "\n",
    "def forward(logits, labels):\n",
    "    sigmoid_logits = torch.sigmoid(logits)\n",
    "    print('sigmoid_logits: ', sigmoid_logits)\n",
    "\n",
    "    smooth_labels = (1.0 - smoothing) * labels + (smoothing / 5.0)\n",
    "    print('smooth_labels: ', smooth_labels)\n",
    "    \n",
    "    log_probs = torch.log(sigmoid_logits)\n",
    "    print('log_probs: ', log_probs)\n",
    "\n",
    "    loss = -torch.sum(smooth_labels * log_probs + (1.0 - smooth_labels) * torch.log(1.0 - sigmoid_logits))\n",
    "    print(loss)\n",
    "    return loss / logits.size(0)\n",
    "\n",
    "loss = forward(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30552b7-e01e-4fa1-ad06-dc26e65e0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([0, 1, 1, 0, 0], dtype=torch.float)\n",
    "logits = torch.tensor([-0.025, 0.4325, 0.665, -0.1594, 0.0043], dtype=torch.float)\n",
    "smoothing = 0.3\n",
    "\n",
    "\n",
    "def forward(logits, labels):\n",
    "    ''' \n",
    "    Forward pass for Label Smoothing + Focal Loss.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits predicted by the model.\n",
    "        labels (torch.Tensor): True labels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Computed Label Smoothing + Focal Loss.\n",
    "    '''\n",
    "\n",
    "    gamma = 2\n",
    "    epsilon = 0.3\n",
    "\n",
    "    sm_labels = (1 - epsilon) * labels + (epsilon / 5)\n",
    "    print('sm_labels: ', sm_labels)\n",
    "\n",
    "    logits_sigmoid = torch.sigmoid(logits)\n",
    "    log_probs = torch.log(logits_sigmoid)\n",
    "    logits_flat = logits_sigmoid.view(-1)\n",
    "    labels_flat = sm_labels.view(-1)\n",
    "\n",
    "    # Focal Loss components\n",
    "    pos_loss = -((1 - logits_flat) ** gamma) * labels_flat * torch.log(logits_flat)\n",
    "    neg_loss = -((logits_flat) ** gamma) * (1 - labels_flat) * torch.log(1 - logits_flat)\n",
    "\n",
    "    # Combine Focal Loss components\n",
    "    # loss = -((1 - logits_flat) ** gamma) * labels_flat * torch.log(logits_flat) - (1 - labels_flat) * (logits_flat ** gamma) * torch.log(1 - logits_flat)\n",
    "\n",
    "    loss = pos_loss + neg_loss\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "# Example usage\n",
    "loss = forward(logits, labels)\n",
    "print('Combined Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f6ea5-e01e-446b-8e37-cf0b2bae0b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f9f2c-1e6e-4732-b004-c0c5fc546449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def smoothed_focal_loss(pt, gamma, epsilon=0.3):\n",
    "    return -(1 - epsilon) * (1 - pt) ** gamma * np.log(pt + 1e-20) - epsilon * (1 - pt) ** gamma * np.log(1 - pt + 1e-20)\n",
    "\n",
    "pt_values = np.linspace(0.01, 1, 100)\n",
    "gamma_values = [0, 1.0, 1.5, 2.0, 2.5]  # list of Gamma values for check\n",
    "epsilon = -0.3  # Smoothing epsilon\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, gamma in enumerate(gamma_values, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    focal_loss_values = focal_loss(pt_values, gamma)\n",
    "    smoothed_focal_loss_values = smoothed_focal_loss(pt_values, gamma, epsilon)\n",
    "    plt.plot(pt_values, focal_loss_values, label=f'Gamma = {gamma} (Original)')\n",
    "    plt.plot(pt_values, smoothed_focal_loss_values, label=f'Gamma = {gamma} (Smoothed)')\n",
    "    plt.title(f'Gamma = {gamma}')\n",
    "    plt.xlabel('f(Q(x)) - Predicted Probability')\n",
    "    plt.ylabel('Focal Loss')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce6173-425e-4457-b3b5-83349f23a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from utils import calculate_metrics\n",
    "\n",
    "# True labels\n",
    "true_labels = [\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0]]\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = [\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0]]\n",
    "\n",
    "# Calculate confusion matrices\n",
    "\n",
    "confusion_matrices = multilabel_confusion_matrix(true_labels, predicted_labels)\n",
    "dq = ['Neoplasms', 'Digestive system diseases', 'Nervous system diseases', 'Cardiovascular diseases', 'General pathological conditions']\n",
    "\n",
    "\n",
    "\n",
    "for i, confusion_matrix in enumerate(confusion_matrices):\n",
    "    label_name = f\"Label {dq[i]}\"\n",
    "    tp = confusion_matrix[1, 1]\n",
    "    fp = confusion_matrix[0, 1]\n",
    "    fn = confusion_matrix[1, 0]\n",
    "\n",
    "    print(f\"{label_name} - TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Flatten true and predicted labels\n",
    "flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
    "flat_predicted_labels = [label for sublist in predicted_labels for label in sublist]\n",
    "\n",
    "# Calculate micro F1 score\n",
    "micro_f1 = f1_score(flat_true_labels, flat_predicted_labels, average='micro')\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = f1_score(flat_true_labels, flat_predicted_labels, average='macro')\n",
    "\n",
    "print(f\"Micro F1 Score: {micro_f1}\")\n",
    "print(f\"Macro F1 Score: {macro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3703eb89-4226-45f2-9b1b-e1e27b725bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_score_micro': 0.5926, 'f1_score_macro': 0.6276, 'accuracy_micro': 0.78, 'accuracy_macro': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from utils import calculate_metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "# True labels\n",
    "true_labels = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0]])\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = np.array([\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0]])\n",
    "\n",
    "# Calculate confusion matrices\n",
    "print(calculate_metrics(true_labels, predicted_labels))\n",
    "# confusion_matrices = multilabel_confusion_matrix(true_labels, predicted_labels)\n",
    "# dq = ['Neoplasms', 'Digestive system diseases', 'Nervous system diseases', 'Cardiovascular diseases', 'General pathological conditions']\n",
    "\n",
    "\n",
    "\n",
    "# for i, confusion_matrix in enumerate(confusion_matrices):\n",
    "#     label_name = f\"Label {dq[i]}\"\n",
    "#     tp = confusion_matrix[1, 1]\n",
    "#     fp = confusion_matrix[0, 1]\n",
    "#     fn = confusion_matrix[1, 0]\n",
    "\n",
    "#     # print(f\"{label_name} - TP: {tp}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "# print(f1_score(true_labels, predicted_labels, average='macro'))\n",
    "\n",
    "# # Flatten true and predicted labels\n",
    "# flat_true_labels = [label for sublist in true_labels for label in sublist]\n",
    "# flat_predicted_labels = [label for sublist in predicted_labels for label in sublist]\n",
    "\n",
    "# # Calculate micro F1 score\n",
    "# micro_f1 = f1_score(flat_true_labels, flat_predicted_labels, average='micro')\n",
    "\n",
    "# # Calculate macro F1 score\n",
    "# macro_f1 = f1_score(flat_true_labels, flat_predicted_labels, average='macro')\n",
    "\n",
    "# print(f\"Micro F1 Score: {micro_f1}\")\n",
    "# print(f\"Macro F1 Score: {macro_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eff5e8-23e9-4e6a-a645-0653620a636a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

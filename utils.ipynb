{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stop words from the NLTK library for the English language.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_multi_sentences(input1, input2):\n",
    "    '''\n",
    "    Calculate Jaccard similarity between two sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - input1 (str): First sentence.\n",
    "    - input2 (str): Second sentence.\n",
    "\n",
    "    Returns:\n",
    "    float: Jaccard similarity score between the two sentences.\n",
    "    '''\n",
    "    sentence1 = set(input1.replace('.', ' ').lower().split())\n",
    "    sentence2 = set(input2.replace('.', ' ').lower().split())\n",
    "    cm = sentence1.intersection(sentence2)\n",
    "    score = float(len(cm)) / (len(sentence1) + len(sentence2) - len(cm))\n",
    "    \n",
    "    return score\n",
    "\n",
    "def parallel_process_data_filter(args):\n",
    "    '''\n",
    "    Parallel processing function to calculate Jaccard similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - args (tuple): Tuple containing (df1, df2, col, i).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing numpy arrays of indices and scores.\n",
    "    '''\n",
    "    df1, df2, col, i = args\n",
    "    idx_1, idx_2, scores = [], [], []\n",
    "    for j in range(len(df2)):\n",
    "        j_score = jaccard_similarity_multi_sentences(df1[col].iloc[i], df2[col].iloc[j])\n",
    "        if j_score > 0.8:\n",
    "            idx_1.append(i)\n",
    "            idx_2.append(j)\n",
    "            scores.append(j_score)\n",
    "    return np.array(idx_1), np.array(idx_2), np.array(scores)\n",
    "\n",
    "def data_filter_parallel(df1, df2, col='medical_abstract', processes=8):\n",
    "    '''\n",
    "    Filter data in parallel based on Jaccard similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (DataFrame): First DataFrame.\n",
    "    - df2 (DataFrame): Second DataFrame.\n",
    "    - col (str): Column name for text data.\n",
    "    - processes (int): Number of parallel processes.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing numpy arrays of indices and scores.\n",
    "    '''\n",
    "    pool = Pool(processes)\n",
    "    func_args = [(df1, df2, col, i) for i in range(len(df1))]\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(func_args), desc=\"Processing\", position=0) as pbar:\n",
    "        for result in pool.imap_unordered(parallel_process_data_filter, func_args):\n",
    "            results.append(result)\n",
    "            pbar.update()\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    idx_1, idx_2, scores = [], [], []\n",
    "    for result in results:\n",
    "        idx_1.extend(result[0])\n",
    "        idx_2.extend(result[1])\n",
    "        scores.extend(result[2])\n",
    "\n",
    "    return np.array(idx_1), np.array(idx_2), np.array(scores)\n",
    "\n",
    "def check_replace_values(score):\n",
    "    '''\n",
    "    Check if all values in the array are the same.\n",
    "\n",
    "    Parameters:\n",
    "    - score (numpy array): Array of values.\n",
    "\n",
    "    Prints:\n",
    "    - Information about whether all values are the same or not.\n",
    "    '''\n",
    "    equal_or_not = np.count_nonzero((score == 1)) == len(score)\n",
    "    if not equal_or_not:\n",
    "        print('Is the same :', equal_or_not, '\\t', 'Other values 1: ', score[score!=1])\n",
    "    else:\n",
    "        print('Is all the same :', equal_or_not, '\\t', 'Total values: ', len(score))\n",
    "\n",
    "def visualize_multilabel(label,\n",
    "                         data,\n",
    "                         title,\n",
    "                         x_title = 'Description',\n",
    "                         color='darkslategray'):\n",
    "    \n",
    "    fig = go.Figure(data=[go.Bar(x=label,\n",
    "                                 y=data,\n",
    "                                 marker_color=color,\n",
    "                                 text=data,\n",
    "                                 textposition='auto')])\n",
    "    fig.update_layout(\n",
    "        # xaxis=dict(tickangle=-30),\n",
    "        xaxis_title=x_title,\n",
    "        yaxis_title='Values',\n",
    "        coloraxis_colorbar=dict(title='Values'),\n",
    "        bargap=0.1,\n",
    "        title=title,\n",
    "        plot_bgcolor='lightgray'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def normalize_special_text(df, column_name, new_col, characters_to_remove=None, char_mapping=None):\n",
    "    \"\"\"\n",
    "    Normalize the specified text column in the DataFrame by removing specified characters and applying mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the text column to normalize.\n",
    "    - column_name: str\n",
    "        The name of the text column to normalize.\n",
    "    - characters_to_remove: list, optional\n",
    "        List of characters to remove from the text.\n",
    "    - char_mapping: dict, optional\n",
    "        Dictionary mapping characters to their normalized replacements.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame\n",
    "        The DataFrame with the normalized text column.\n",
    "    \"\"\"\n",
    "        \n",
    "    characters_to_remove = characters_to_remove or ['&', '$', '!', '?', '(', ')', '[', ']', '\"', \"'\", '=', ':', '`', ',', '-', '.']\n",
    "    char_mapping = char_mapping or {'&': '', '$': '', '!': '', '?': '', '(': '', ')': '', '[': '',\n",
    "                                    ']': '', '\"': '', \"'\": '', '=': ' ', ':': '', '`': '', ',': '', '-': ' ', '.':''}\n",
    "\n",
    "    pattern = re.compile('|'.join(re.escape(char) for char in characters_to_remove))\n",
    "    df[new_col] = df[column_name].apply(lambda x: pattern.sub(lambda m: char_mapping.get(m.group(0), ''), x.lower()))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r\"\\s+\", \" \", x).strip())   # Remove extra whitespaces\n",
    "    return df\n",
    "\n",
    "def remove_stopwords(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove stop words from a text column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the text column.\n",
    "    - column_name: str\n",
    "        The name of the text column to remove stop words from.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame\n",
    "        The DataFrame with stop words removed from the specified text column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the set of English stop words from the NLTK library\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Apply the stop word removal process to the specified text column\n",
    "    df[column_name] = df[column_name].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def show_wordCount(df, idx, lbl):\n",
    "    \"\"\"\n",
    "    Generate a histogram and print statistics for word count distribution for a specific condition.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the text data and condition labels.\n",
    "    - idx: int\n",
    "        The condition label for which the word count distribution is to be visualized.\n",
    "    - lbl: str\n",
    "        The label corresponding to the condition for better identification in the plot title.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    df['word_count'][df['condition_label'] == idx].iplot(\n",
    "        kind='hist',\n",
    "        bins=150,\n",
    "        xTitle='Text length',\n",
    "        linecolor='black',\n",
    "        color='red',\n",
    "        yTitle='Vocabulary Frequency',\n",
    "        title=f'{lbl} words count distribution'\n",
    "    )\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Max word count for {lbl}: {max(df[\"word_count\"][df.condition_label==idx])}')\n",
    "    print(f'Min word count for {lbl}: {min(df[\"word_count\"][df.condition_label==idx])}')\n",
    "\n",
    "\n",
    "def get_quartile_range(df, col, id_list):\n",
    "    \"\"\"\n",
    "    Generate box plots to show the quartile range of word count distribution for multiple conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the text data and condition labels.\n",
    "    - id_list: list\n",
    "        List of condition labels for which box plots will be generated.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    color_list = ['blueviolet', 'olive', 'lightgreen', 'hotpink', 'red']\n",
    "    \n",
    "    # Create box plots\n",
    "    trace = [go.Box(y=df[col][df['condition_label']==idx],\n",
    "                    name=f'Condition {class_mapping[str(idx)]}',\n",
    "                    marker=dict(color=color_list[idx])\n",
    "                    ) for idx in id_list]\n",
    "\n",
    "    layout = go.Layout(title=\"Length of the text\", xaxis=dict(title='Condition Label'), yaxis=dict(title='Word Count'))\n",
    "    fig = go.Figure(data=trace, layout=layout)\n",
    "    iplot(fig, filename=\"Quartile of word count distribution\")\n",
    "    \n",
    "def visualize_wordcloud(text_clean, ax, title, max_words):\n",
    "    \"\"\"\n",
    "    Generate and visualize a word cloud from cleaned text data.\n",
    "\n",
    "    Parameters:\n",
    "    - text_clean: list\n",
    "        List of cleaned text data.\n",
    "    - ax: matplotlib.axes._subplots.AxesSubplot\n",
    "        Matplotlib axis on which the word cloud will be visualized.\n",
    "    - title: str\n",
    "        Title for the word cloud visualization.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create a WordCloud object with specified settings\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        width=480,\n",
    "        height=480,\n",
    "        max_words=max_words\n",
    "    ).generate(\" \".join(text_clean))\n",
    "\n",
    "    # Display the word cloud on the provided axis\n",
    "    ax.imshow(wordcloud)\n",
    "    \n",
    "    # Turn off axis labels\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Set the title for the word cloud visualization\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "def get_clean_text(data):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data by joining and splitting.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list\n",
    "        List of text data to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    - list\n",
    "        Cleaned and preprocessed text data.\n",
    "    \"\"\"\n",
    "    # Join the text data into a single string and then split into a list of words\n",
    "    data = ' '.join(data).strip().split(' ')\n",
    "    return data\n",
    "\n",
    "def get_top_n_words(df, cols, n=None):\n",
    "    \"\"\"\n",
    "    Get the top N most frequent words from a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the preprocessed text column.\n",
    "    - column_name: str\n",
    "        The name of the column containing preprocessed text (without stop words).\n",
    "    - n: int, optional\n",
    "        Number of top words to retrieve. If None, return all words.\n",
    "\n",
    "    Returns:\n",
    "    - list\n",
    "        A list of tuples containing the top N words and their frequencies.\n",
    "    \"\"\"\n",
    "    # Extract the preprocessed text column from the DataFrame\n",
    "    corpus = df[cols].tolist()\n",
    "\n",
    "    vectorizer = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vectorizer.transform(corpus)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "\n",
    "    # Create a list of tuples containing word and frequency\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n]\n",
    "\n",
    "def find_common_words(words_freq_lists, relative_n = 3):\n",
    "    \"\"\"\n",
    "    Find common words that appear in at least 3 out of 5 words_freq lists.\n",
    "\n",
    "    Parameters:\n",
    "    - words_freq_lists: list\n",
    "        List of words_freq lists, each containing tuples of (word, frequency).\n",
    "\n",
    "    Returns:\n",
    "    - list\n",
    "        List of common words meeting the specified criteria.\n",
    "    \"\"\"\n",
    "    sets_of_words = [list(zip(*words_freq))[0] for words_freq in words_freq_lists]\n",
    "    combined_list = [value for tpl in sets_of_words for value in tpl]\n",
    "\n",
    "    word_counts = Counter(combined_list)\n",
    "    rare_words = {key: '' for key, value in word_counts.items() if value >= 3}\n",
    "    return rare_words\n",
    "\n",
    "def replace_words(text, replacements):\n",
    "    pattern = re.compile(r'\\b(?:%s)\\b' % '|'.join(map(re.escape, replacements.keys())), re.IGNORECASE)\n",
    "    return pattern.sub(lambda x: replacements[x.group().lower()], text)\n",
    "\n",
    "def visualization_top_word_count(x_data, y_data, text_data, color=None, title=None, top_n=None):\n",
    "    \"\"\"\n",
    "    Generate a bar chart using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    - x_data: list\n",
    "        X-axis data.\n",
    "    - y_data: list\n",
    "        Y-axis data.\n",
    "    - text_data: list\n",
    "        Text data for hover text.\n",
    "    - color: str, optional\n",
    "        Bar color.\n",
    "    - title: str, optional\n",
    "        Chart title.\n",
    "    - top_n: int, optional\n",
    "        Number of top words to display.\n",
    "\n",
    "    Returns:\n",
    "    - go.Figure\n",
    "        Plotly Figure object.\n",
    "    \"\"\"\n",
    "    if top_n:\n",
    "        x_data = x_data[:top_n]\n",
    "        y_data = y_data[:top_n]\n",
    "        text_data = text_data[:top_n]\n",
    "\n",
    "    fig = go.Figure([go.Bar(x=x_data, y=y_data, text=text_data, marker_color=color)])\n",
    "    fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title_text=title)\n",
    "    # fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "    return fig\n",
    "\n",
    "def stemming_lemma_reprocess(text, type_select='stemming'):\n",
    "    \"\"\"\n",
    "    Preprocesses text by applying stemming or lemmatization.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Input text to be preprocessed.\n",
    "    - type_select (str, optional): Type of preprocessing to apply. \n",
    "      Options: 'stemming' for stemming, 'lemma' for lemmatization.\n",
    "      Default is 'stemming'.\n",
    "\n",
    "    Returns:\n",
    "    - str: Processed text after applying stemming or lemmatization.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Apply stemming or lemmatization based on type_select\n",
    "    if type_select == 'stemming':\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "    elif type_select == 'lemma':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    else:\n",
    "        # Raise an error for invalid type_select values\n",
    "        raise ValueError('type_select must be either \"stemming\" or \"lemma\"')\n",
    "        \n",
    "        \n",
    "def tokenize_and_format(data, direction_model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tokenizes and formats the input data for training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of tuples containing text data and corresponding labels.\n",
    "        direction_model (str): Path or identifier of the pretrained model.\n",
    "        device (str, optional): Device to which tensors are moved ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tokenized and formatted inputs for the model.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer for the specified pretrained model\n",
    "    tokenizer = BertTokenizer.from_pretrained(direction_model)\n",
    "\n",
    "    # Tokenize the input text data\n",
    "    inputs = tokenizer(\n",
    "        [item for item in data],\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    # Return a dictionary containing the tokenized and formatted inputs\n",
    "    return tokenizer, ({\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'token_type_ids': inputs['token_type_ids']\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

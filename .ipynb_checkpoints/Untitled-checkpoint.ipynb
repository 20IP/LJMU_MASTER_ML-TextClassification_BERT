{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0b8202-4041-45a8-8d39-a86af7d920a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df80c09f-34c0-4e4e-a96b-b8f4b9800007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1]</td>\n",
       "      <td>Đây là bệnh về da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>Da thâm nổi mụm và tiền ung thư</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0]</td>\n",
       "      <td>Nhóm bệnh ung thư về sắc tố</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4]</td>\n",
       "      <td>Cơ xương có thể không phát triển</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3]</td>\n",
       "      <td>Gan nhiễm mỡ và có dấu hiệu xơ gan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[3, 0]</td>\n",
       "      <td>Xơ gan giai đoạn tiền phát nguy cơ gây ung thư...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lbl                                               text\n",
       "0     [1]                                  Đây là bệnh về da\n",
       "1  [1, 0]                    Da thâm nổi mụm và tiền ung thư\n",
       "2     [0]                        Nhóm bệnh ung thư về sắc tố\n",
       "3     [4]                   Cơ xương có thể không phát triển\n",
       "4     [3]                 Gan nhiễm mỡ và có dấu hiệu xơ gan\n",
       "5  [3, 0]  Xơ gan giai đoạn tiền phát nguy cơ gây ung thư..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = {\"lbl\":[[1], [1,0], [0], [4], [3], [3,0]], \n",
    "                          \"text\": ['Đây là bệnh về da',\n",
    "                                   'Da thâm nổi mụm và tiền ung thư',\n",
    "                                   'Nhóm bệnh ung thư về sắc tố',\n",
    "                                   'Cơ xương có thể không phát triển',\n",
    "                                   'Gan nhiễm mỡ và có dấu hiệu xơ gan',\n",
    "                                   'Xơ gan giai đoạn tiền phát nguy cơ gây ung thư gan']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe52371-967f-447d-9351-7c7d59680b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbl</th>\n",
       "      <th>text</th>\n",
       "      <th>binary_lbl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1]</td>\n",
       "      <td>Đây là bệnh về da</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>Da thâm nổi mụm và tiền ung thư</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0]</td>\n",
       "      <td>Nhóm bệnh ung thư về sắc tố</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4]</td>\n",
       "      <td>Cơ xương có thể không phát triển</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3]</td>\n",
       "      <td>Gan nhiễm mỡ và có dấu hiệu xơ gan</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lbl                                text       binary_lbl\n",
       "0     [1]                   Đây là bệnh về da  [0, 1, 0, 0, 0]\n",
       "1  [1, 0]     Da thâm nổi mụm và tiền ung thư  [1, 1, 0, 0, 0]\n",
       "2     [0]         Nhóm bệnh ung thư về sắc tố  [1, 0, 0, 0, 0]\n",
       "3     [4]    Cơ xương có thể không phát triển  [0, 0, 0, 0, 1]\n",
       "4     [3]  Gan nhiễm mỡ và có dấu hiệu xơ gan  [0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_cvt(labels, max_val=4):\n",
    "    zeros_arr = [0]*(max_val+1)\n",
    "    for label in labels:\n",
    "        zeros_arr[label] = 1\n",
    "    return zeros_arr\n",
    "\n",
    "\n",
    "df['binary_lbl'] = df.lbl.apply(lambda x: binary_cvt(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2356100-055b-455e-8bac-0d7e647cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropyLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Cross Entropy Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Cross Entropy Loss for addressing multi-label classification tasks.\n",
    "    It uses PyTorch's built-in CrossEntropyLoss, adjusted for multi-label.\n",
    "\n",
    "    Attributes:\n",
    "        loss_fn (torch.nn.CrossEntropyLoss): PyTorch Cross Entropy Loss instance.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossMultiLabel, self).__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ''' \n",
    "        Forward pass for Cross Entropy Loss.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Logits predicted by the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Cross Entropy Loss.\n",
    "        '''\n",
    "        # Apply sigmoid activation to logits for multi-label classification\n",
    "        logits_sigmoid = torch.sigmoid(logits)\n",
    "\n",
    "        # Flatten the logits and labels for multi-label loss calculation\n",
    "        logits_flat = logits_sigmoid.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        # Binary cross entropy loss\n",
    "        loss = F.binary_cross_entropy(logits_flat, labels_flat)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class FocalLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Focal Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Focal Loss for addressing class imbalance in multi-label classification tasks.\n",
    "    It introduces a modulating factor (gamma) to down-weight easy samples.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Modulating factor for Focal Loss.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLossMultiLabel, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Focal Loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Raw outputs from the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Focal Loss.\n",
    "        '''\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "class FocalLossWithBatchNormL2MultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Focal Loss with BatchNorm L2 Penalty for Multi-Label Classification\n",
    "    \n",
    "    This class defines Focal Loss with an additional BatchNorm L2 penalty for multi-label classification.\n",
    "    It helps prevent overfitting by penalizing large weights in BatchNorm layers.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Modulating factor for Focal Loss.\n",
    "        beta (float): Coefficient for BatchNorm L2 penalty.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma=2.0, beta=1e-4):\n",
    "        super(FocalLossWithBatchNormL2MultiLabel, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Focal Loss with BatchNorm L2 Penalty.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Raw outputs from the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Focal Loss with BatchNorm L2 Penalty.\n",
    "        '''\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean() + self.beta * self.batch_norm_l2_penalty()\n",
    "\n",
    "    def batch_norm_l2_penalty(self):\n",
    "        ''' \n",
    "        Compute BatchNorm L2 Penalty.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: L2 penalty for BatchNorm layers.\n",
    "        '''\n",
    "        l2_penalty = torch.tensor(0.0, requires_grad=True)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                l2_penalty += (module.weight ** 2).sum()\n",
    "        return l2_penalty\n",
    "    \n",
    "class LabelSmoothingLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Label Smoothing Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Label Smoothing Loss for addressing multi-label classification tasks.\n",
    "    It mitigates overconfidence in the model predictions by introducing label smoothing.\n",
    "\n",
    "    Attributes:\n",
    "        smoothing (float): Smoothing factor for label smoothing.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingLossMultiLabel, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Label Smoothing Loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Logits predicted by the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Label Smoothing Loss.\n",
    "        '''\n",
    "        sigmoid_outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        smooth_labels = (1.0 - self.smoothing) * labels + self.smoothing / 2.0\n",
    "        log_probs = torch.log(sigmoid_outputs)\n",
    "\n",
    "        loss = -torch.sum(smooth_labels * log_probs + (1.0 - smooth_labels) * torch.log(1.0 - sigmoid_outputs))\n",
    "        return loss / outputs.size(0)  # Normalize by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27083a1c-ea04-47b7-816e-07b3f271f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 10:56:52.045606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-02-21 10:56:52.045669: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss  # For multi-label classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59983142-5fbf-4514-a9bd-8e982962e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5259, grad_fn=<DivBackward0>)\n",
      "tensor(3.6424, grad_fn=<DivBackward0>)\n",
      "tensor(3.2496, grad_fn=<DivBackward0>)\n",
      "tensor(3.0011, grad_fn=<DivBackward0>)\n",
      "tensor(3.4814, grad_fn=<DivBackward0>)\n",
      "tensor(3.2179, grad_fn=<DivBackward0>)\n",
      "tensor(3.1448, grad_fn=<DivBackward0>)\n",
      "tensor(3.0556, grad_fn=<DivBackward0>)\n",
      "tensor(2.9386, grad_fn=<DivBackward0>)\n",
      "tensor(2.9808, grad_fn=<DivBackward0>)\n",
      "tensor(2.8337, grad_fn=<DivBackward0>)\n",
      "tensor(2.7732, grad_fn=<DivBackward0>)\n",
      "tensor(2.7289, grad_fn=<DivBackward0>)\n",
      "tensor(2.8315, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 63\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/devPy3.8/lib/python3.8/site-packages/torch/optim/adamw.py:313\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 313\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "texts = df.text.tolist()\n",
    "\n",
    "labels = torch.tensor(df.binary_lbl.tolist())\n",
    "\n",
    "# Token hóa văn bản\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tạo dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.tokenized_texts['input_ids'][idx], 'attention_mask': self.tokenized_texts['attention_mask'][idx], 'labels': self.labels[idx]}\n",
    "\n",
    "dataset = CustomDataset(tokenized_texts, labels)\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 2\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Tạo mô hình BERT\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Hàm mất mát BCEWithLogitsLoss\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = LabelSmoothingLossMultiLabel()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tối ưu hóa\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Quá trình huấn luyện\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Tính logits từ mô hình\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Tính toán loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "        print(loss)\n",
    "        # Backpropagation và tối ưu hóa\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá mô hình (tương tự trong quá trình huấn luyện)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52be83c-4107-4e52-81d4-8d8f736c45a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

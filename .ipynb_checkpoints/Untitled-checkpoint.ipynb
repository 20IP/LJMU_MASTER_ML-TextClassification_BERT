{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b8202-4041-45a8-8d39-a86af7d920a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80c09f-34c0-4e4e-a96b-b8f4b9800007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {\"lbl\":[[1], [1,0], [0], [4], [3], [3,0]], \n",
    "                          \"text\": ['Đây là bệnh về da',\n",
    "                                   'Da thâm nổi mụm và tiền ung thư',\n",
    "                                   'Nhóm bệnh ung thư về sắc tố',\n",
    "                                   'Cơ xương có thể không phát triển',\n",
    "                                   'Gan nhiễm mỡ và có dấu hiệu xơ gan',\n",
    "                                   'Xơ gan giai đoạn tiền phát nguy cơ gây ung thư gan']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe52371-967f-447d-9351-7c7d59680b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cvt(labels, max_val=4):\n",
    "    zeros_arr = [0]*(max_val+1)\n",
    "    for label in labels:\n",
    "        zeros_arr[label] = 1\n",
    "    return zeros_arr\n",
    "\n",
    "\n",
    "df['binary_lbl'] = df.lbl.apply(lambda x: binary_cvt(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2356100-055b-455e-8bac-0d7e647cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossEntropyLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Cross Entropy Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Cross Entropy Loss for addressing multi-label classification tasks.\n",
    "    It uses PyTorch's built-in CrossEntropyLoss, adjusted for multi-label.\n",
    "\n",
    "    Attributes:\n",
    "        loss_fn (torch.nn.CrossEntropyLoss): PyTorch Cross Entropy Loss instance.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossMultiLabel, self).__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ''' \n",
    "        Forward pass for Cross Entropy Loss.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Logits predicted by the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Cross Entropy Loss.\n",
    "        '''\n",
    "        # Apply sigmoid activation to logits for multi-label classification\n",
    "        logits_sigmoid = torch.sigmoid(logits)\n",
    "\n",
    "        # Flatten the logits and labels for multi-label loss calculation\n",
    "        logits_flat = logits_sigmoid.view(-1)\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        # Binary cross entropy loss\n",
    "        loss = F.binary_cross_entropy(logits_flat, labels_flat)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class FocalLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Focal Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Focal Loss for addressing class imbalance in multi-label classification tasks.\n",
    "    It introduces a modulating factor (gamma) to down-weight easy samples.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Modulating factor for Focal Loss.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLossMultiLabel, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Focal Loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Raw outputs from the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Focal Loss.\n",
    "        '''\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "class FocalLossWithBatchNormL2MultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Focal Loss with BatchNorm L2 Penalty for Multi-Label Classification\n",
    "    \n",
    "    This class defines Focal Loss with an additional BatchNorm L2 penalty for multi-label classification.\n",
    "    It helps prevent overfitting by penalizing large weights in BatchNorm layers.\n",
    "\n",
    "    Attributes:\n",
    "        gamma (float): Modulating factor for Focal Loss.\n",
    "        beta (float): Coefficient for BatchNorm L2 penalty.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma=2.0, beta=1e-4):\n",
    "        super(FocalLossWithBatchNormL2MultiLabel, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Focal Loss with BatchNorm L2 Penalty.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Raw outputs from the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Focal Loss with BatchNorm L2 Penalty.\n",
    "        '''\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(outputs, labels, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean() + self.beta * self.batch_norm_l2_penalty()\n",
    "\n",
    "    def batch_norm_l2_penalty(self):\n",
    "        ''' \n",
    "        Compute BatchNorm L2 Penalty.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: L2 penalty for BatchNorm layers.\n",
    "        '''\n",
    "        l2_penalty = torch.tensor(0.0, requires_grad=True)\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                l2_penalty += (module.weight ** 2).sum()\n",
    "        return l2_penalty\n",
    "    \n",
    "class LabelSmoothingLossMultiLabel(nn.Module):\n",
    "    ''' \n",
    "    Label Smoothing Loss for Multi-Label Classification\n",
    "    \n",
    "    This class defines the Label Smoothing Loss for addressing multi-label classification tasks.\n",
    "    It mitigates overconfidence in the model predictions by introducing label smoothing.\n",
    "\n",
    "    Attributes:\n",
    "        smoothing (float): Smoothing factor for label smoothing.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingLossMultiLabel, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        ''' \n",
    "        Forward pass for Label Smoothing Loss.\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Logits predicted by the model.\n",
    "            labels (torch.Tensor): True labels.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Label Smoothing Loss.\n",
    "        '''\n",
    "        sigmoid_outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        smooth_labels = (1.0 - self.smoothing) * labels + self.smoothing / 2.0\n",
    "        log_probs = torch.log(sigmoid_outputs)\n",
    "\n",
    "        loss = -torch.sum(smooth_labels * log_probs + (1.0 - smooth_labels) * torch.log(1.0 - sigmoid_outputs))\n",
    "        return loss / outputs.size(0)  # Normalize by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26429b84-4a70-489a-a738-f0e7054ba070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Medical-Abstracts-TC-Corpus/preprocessed-medical_tc_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27083a1c-ea04-47b7-816e-07b3f271f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import BCEWithLogitsLoss  # For multi-label classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59983142-5fbf-4514-a9bd-8e982962e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Chuẩn bị dữ liệu\n",
    "texts = df.medical_abstract.tolist()\n",
    "labels = torch.tensor(df[['neoplasms','digestive','nervous','cardiovascular','general']].values)\n",
    "# Token hóa văn bản\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Tạo dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.tokenized_texts['input_ids'][idx], 'attention_mask': self.tokenized_texts['attention_mask'][idx], 'labels': self.labels[idx]}\n",
    "\n",
    "dataset = CustomDataset(tokenized_texts, labels)\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Tạo mô hình BERT\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Hàm mất mát BCEWithLogitsLoss\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = CrossEntropyLossMultiLabel()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Tối ưu hóa\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Quá trình huấn luyện\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Tính logits từ mô hình\n",
    "        outputs = model(inputs, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        print(logits)\n",
    "        # Tính toán loss\n",
    "        loss = criterion(logits, labels.float())\n",
    "        print(loss)\n",
    "        # Backpropagation và tối ưu hóa\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá mô hình (tương tự trong quá trình huấn luyện)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59ad2f-8b9f-4ab2-894a-e8dda87a8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(true_labels, predicted_labels, threshold=0.5, report_methods=['micro', 'macro']):\n",
    "    # Convert probability scores to binary predictions based on the threshold\n",
    "    binary_predictions = (predicted_labels > threshold).astype(int)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if 'micro' in report_methods:\n",
    "        # Micro-Averaging: Calculate metrics globally across all classes\n",
    "        tp = np.sum((true_labels == 1) & (binary_predictions == 1))\n",
    "        fp = np.sum((true_labels == 0) & (binary_predictions == 1))\n",
    "        fn = np.sum((true_labels == 1) & (binary_predictions == 0))\n",
    "        tn = np.sum((true_labels == 0) & (binary_predictions == 0))\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['f1_score'] = f1_score\n",
    "        metrics['accuracy'] = accuracy\n",
    "\n",
    "    if 'macro' in report_methods:\n",
    "        # Macro-Averaging: Calculate metrics independently for each class and then average\n",
    "        num_classes = true_labels.shape[1]\n",
    "        precision_macro = recall_macro = f1_score_macro = accuracy_macro = 0\n",
    "\n",
    "        for class_idx in range(num_classes):\n",
    "            tp = np.sum((true_labels[:, class_idx] == 1) & (binary_predictions[:, class_idx] == 1))\n",
    "            fp = np.sum((true_labels[:, class_idx] == 0) & (binary_predictions[:, class_idx] == 1))\n",
    "            fn = np.sum((true_labels[:, class_idx] == 1) & (binary_predictions[:, class_idx] == 0))\n",
    "            tn = np.sum((true_labels[:, class_idx] == 0) & (binary_predictions[:, class_idx] == 0))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "\n",
    "            precision_macro += precision\n",
    "            recall_macro += recall\n",
    "            f1_score_macro += f1_score\n",
    "            accuracy_macro += accuracy\n",
    "\n",
    "        precision_macro /= num_classes\n",
    "        recall_macro /= num_classes\n",
    "        f1_score_macro /= num_classes\n",
    "        accuracy_macro /= num_classes\n",
    "\n",
    "        metrics['precision_macro'] = precision_macro\n",
    "        metrics['recall_macro'] = recall_macro\n",
    "        metrics['f1_score_macro'] = f1_score_macro\n",
    "        metrics['accuracy_macro'] = accuracy_macro\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52be83c-4107-4e52-81d4-8d8f736c45a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score: {'precision_macro': 0.5933333333333334, 'recall_macro': 1.0, 'f1_score_macro': 0.711111111111111, 'accuracy_macro': 0.85}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample true labels\n",
    "true_labels = torch.tensor([[0., 1., 0., 0., 0.],\n",
    "                            [0., 0., 0., 1., 0.],\n",
    "                            [0., 0., 0., 1., 1.],\n",
    "                            [0., 0., 1., 1., 0.],\n",
    "                            [1., 0., 0., 0., 0.],\n",
    "                            [0., 0., 0., 0., 1.],\n",
    "                            [0., 0., 0., 0., 1.],\n",
    "                            [0., 0., 0., 1., 0.]])\n",
    "\n",
    "# Sample predicted labels (binary predictions)\n",
    "predicted_labels = torch.tensor([[0., 1., 0., 0., 0.],\n",
    "                                 [1., 0., 0., 1., 0.],\n",
    "                                 [0., 0., 0., 1., 1.],\n",
    "                                 [0., 0., 1., 1., 0.],\n",
    "                                 [1., 0., 0., 0., 0.],\n",
    "                                 [0., 1., 0., 0., 1.],\n",
    "                                 [1., 1., 1., 1., 1.],\n",
    "                                 [0., 0., 0., 1., 0.]])\n",
    "\n",
    "# Flatten the tensors for scikit-learn compatibility\n",
    "true_labels_flat = true_labels.numpy()\n",
    "predicted_labels_flat = predicted_labels.numpy()\n",
    "\n",
    "micro_f1 = calculate_metrics(true_labels_flat, predicted_labels_flat, report_methods='macro')\n",
    "\n",
    "print(\"Micro F1 Score:\", micro_f1)\n",
    "# Micro F1 Score: 0.7692307692307693\n",
    "# Micro F1 Score: {'precision': 0.625, 'recall': 1.0, 'f1_score': 0.7692307692307693, 'accuracy': 0.85}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72c25d-4cb8-4e93-8112-fcaf8d641898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming you have batches of true labels and predicted probabilities\n",
    "# Sample true labels for two batches\n",
    "true_labels_batch1 = torch.tensor([[0., 1., 0., 0., 0.],\n",
    "                                   [0., 0., 0., 1., 0.]])\n",
    "\n",
    "true_labels_batch2 = torch.tensor([[0., 0., 0., 1., 1.],\n",
    "                                   [0., 0., 1., 1., 0.]])\n",
    "\n",
    "# Sample predicted probabilities for two batches\n",
    "predicted_probs_batch1 = torch.tensor([[0.1, 0.9, 0.3, 0.6, 0.7],\n",
    "                                       [0.8, 0.2, 0.4, 0.7, 0.1]])\n",
    "\n",
    "predicted_probs_batch2 = torch.tensor([[0.2, 0.4, 0.5, 0.6, 0.9],\n",
    "                                       [0.3, 0.7, 0.8, 0.2, 0.6]])\n",
    "\n",
    "# Flatten the tensors for scikit-learn compatibility\n",
    "true_labels_all = torch.cat([true_labels_batch1, true_labels_batch2], dim=0).view(-1).numpy()\n",
    "predicted_probs_all = torch.cat([predicted_probs_batch1, predicted_probs_batch2], dim=0).view(-1).numpy()\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(true_labels_all, predicted_probs_all)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a457c-8bfc-468e-b6a5-4173914b560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b5e86-9956-4ca2-ab7f-5ad78ba311d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.array([0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87641ec5-2f3d-4c74-8e07-a371e73af59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 26/40\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953426e-09d2-4c32-b569-3befe42c6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(a1, a2)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825f1e2-116c-42b7-9d68-30c8ee47d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o=0\n",
    "for i in zip(a1, a2):\n",
    "    if i[0] == i[1]:\n",
    "        o+=1\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349261b-0a9f-4704-9f5b-58e70ada796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a1 = np.array([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=np.int32)\n",
    "a2 = np.array([0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.], dtype=np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78834e42-9ec9-48f6-9802-f4c6d882fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.concatenate([a1,a2])\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392428f3-6491-4cf8-bf64-196be45b2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ca871-0032-4986-a1f9-e8804353ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given matrix\n",
    "matrix = torch.tensor([[-0.8426, 0.4689, -0.8778, 0.3294, -0.3936],\n",
    "                       [-0.0423, 0.6215, -0.2281, 0.2345, -0.3712]])\n",
    "\n",
    "lbl_true = torch.tensor([[0,0,1,0,0],\n",
    "                         [0,1,0,1,0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9629197-c320-4f3a-9524-e29160946ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create an empty tensor\n",
    "empty_tensor = torch.empty(0)  # Assuming 5 columns, adjust as needed\n",
    "\n",
    "# Create a new tensor\n",
    "new_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "# Concatenate the empty tensor with the new tensor along dimension 0\n",
    "empty_tensor = torch.cat([empty_tensor, new_tensor],dim=0)\n",
    "\n",
    "print(\"\\nConcatenated Tensor:\")\n",
    "print(empty_tensor)\n",
    "\n",
    "\n",
    "empty_tensor = torch.cat([empty_tensor, new_tensor])\n",
    "\n",
    "print(\"\\nConcatenated Tensor:\")\n",
    "print(empty_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84cc75-007f-4283-a9d4-7d281b9e8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given matrix\n",
    "matrix = torch.tensor([[-0.8426, 0.4689, 0.8778, 0.3294, -0.3936],\n",
    "                       [-0.00423, 0.8, -0.02281, 0.9, -0.03712]])\n",
    "\n",
    "# True labels\n",
    "lbl_true = torch.tensor([[0, 0, 1, 0, 0],\n",
    "                         [0, 1, 0, 1, 0]])\n",
    "\n",
    "# Apply sigmoid activation to the matrix\n",
    "matrix_sigmoid = torch.sigmoid(matrix)\n",
    "\n",
    "# Calculate BCEWithLogitsLoss\n",
    "loss = F.binary_cross_entropy_with_logits(matrix_sigmoid, lbl_true.float())\n",
    "\n",
    "print(\"BCEWithLogitsLoss:\")\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c0f9f-bbfe-4e4b-9ec3-ea56c87a1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your original dataset\n",
    "original_data = {'Column1': [9, 9, 9, 9],\n",
    "                 'Column2': [9, 9, 9, 8]\n",
    "                }\n",
    "\n",
    "# Create a DataFrame from your original dataset\n",
    "df = pd.DataFrame(original_data)\n",
    "\n",
    "# Your list of values\n",
    "LB = [[0, 0, 0, 1], [0, 1, 0, 0], [1, 0, 0, 1]]\n",
    "\n",
    "# Convert the list of values to a DataFrame\n",
    "LB_df = pd.DataFrame(LB, columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "# Concatenate the original DataFrame with the new DataFrame\n",
    "result_df = pd.concat([df, LB_df], axis=1)\n",
    "\n",
    "# Display the result\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022961a4-dfb3-47d5-8062-072c1657f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
